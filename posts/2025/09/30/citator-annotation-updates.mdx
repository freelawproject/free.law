---
title: "Citator Expert Annotation Project: A Status Update"
date: "2025-09-30"
tags:
 - "Free Law Project"
 - "Citator"
 - "Opinions"
 - "Case Law"
 - "Artificial Intelligence"
 - "AI"
author: "Rachel Gao"
excerpt: "With an overwhelming response from the legal community, we're in the home stretch of curating the data for building the first open-source AI-powered legal citator—and we need your help to cross the finish line."

---

<p className="lead">A few months ago, we put out a call for help with the Citator Expert Annotation Project—asking legal experts to contribute their knowledge in creating the dataset that will power the first open-source AI legal citator and create the first open-source benchmark dataset of its kind. The response blew us away! Legal professionals from all over have contributed their expertise, and thanks to this incredible support from the community, we're now in the home stretch to get this exciting project over its finish line.</p>

## Defining the Treatment

When defining the treatments, we dug into academic research, looked at the existing commercial Citators in the market, and connected with legal researchers who specialized in this exact area. 

We ultimately created the below treatment matrix, breaking the treatments down by severity and by case history. Direct history are for cases that belong to the same appellate chain while citing references are for those do not belong to the same appellate chain.

<span className="flex flex-col gap-10">
<CaptionedImage
  imgProps={{
    src: "/images/citator/treatments.png",
    alt: "Citator treatments"
  }}
  border="border border-gray-400"
>**Image:** Free Law Project Citator Treatments.</CaptionedImage>
</span>

We intentionally focused exclusively on negative treatments as our citator is designed to flag the stuff you really want to be cautious about—the cases that have been questioned, criticized, or overruled. But here's the thing: legal analysis isn't always black and white. Two lawyers can look at the same citation and disagree on how to characterize it. That's why we added an "Ambiguous" category for those edge cases where even our experts couldn't reach consensus. Furthermore, every citation is reviewed by at least two volunteers to minimize bias.

We also included an "Unverifiable" category for situations where our AI model can't make a confident call or when its prediction lacks solid supporting evidence. We want to flag uncertainty than to mislead!

## Annotation Status Update

Since we put out the call, nearly 80 legal professionals have reached out wanting to help and we are extremely lucky to have 30 highly experienced experts diligently reading case laws and identifying negative treatments within for the past few months. These legal experts are legal librarians, lawyers, counsels, and court clerks with extensive legal research and analysis experience, and we cannot thank them enough!

We kicked things off with Supreme Court cases—seemed like the logical place to start given SCOTUS's authority and the likelihood of finding negative treatments in their opinions. So far, our volunteers have worked through more than 180 SCOTUS cases, analyzing over 7,600 citations spanning different years and areas of law.

Here's where we stand: We're about 70% done with the second round of annotations for these SCOTUS cases, and the third tie-breaker round is already underway. The tie-breaker round happens when the first two experts disagree on a treatment and a third expert steps in to make the final call. 

To date, we've got close to 4,000 citations from nearly 100 SCOTUS cases that have been through multiple rounds of expert review and now have final treatments assigned.

And we're not stopping at SCOTUS. We've expanded into federal appellate and district courts, state courts, and bankruptcy courts. About 1,000 citations from over 30 federal court cases have completed their first round of annotations, along with nearly 300 citations from 10 state court cases. The rest are in progress, with second-round reviews moving forward as we speak.

## Looking Ahead to the Finish Line

We're now in the final stretch, and we couldn't have gotten here without the incredible volunteers who've dedicated their time and expertise to this project. Every opinion reviewed, every treatment debated, every tie-breaker resolved—it all adds up to something bigger than any one organization could build alone. 

This isn't just about powering the first open-source AI legal citator (though that's pretty huge and we are very excited about it), this will also create the first open-source benchmark dataset of its kind—a resource that will benefit researchers, developers, and legal tech innovators for years to come.

But we're not quite there yet. We need your help to cross the finish line.

If you're a legal professional with research experience and want to be part of this groundbreaking project, we'd love to have you. Fill out our [Google form]((https://docs.google.com/forms/d/1DBtOGFmhfuPn7BixAvuzlqo7h3Gbb5P8Z9nOTDJD2PU/edit)) and let us know your availability—we're happy to work with your schedule. Whether you can contribute a few hours or a few dozen, every bit helps. And as a small token of our appreciation, we're raffling off merch credits to our volunteers throughout the project!

This is a community effort in the truest sense. With your help, we can make open-source legal research tools a reality. Let's get this done together!

